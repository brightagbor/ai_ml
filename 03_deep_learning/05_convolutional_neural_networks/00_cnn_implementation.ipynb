{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12973461",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70437d6e",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd224d4-78e8-40cf-a22b-44351754b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 12:32:46.036798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 12:32:46.091702: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 12:32:46.108307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 12:32:46.146256: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-31 12:32:49.188518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b44cae-085b-4675-a7e1-f5ea85ada610",
   "metadata": {},
   "source": [
    "#### Print the version of tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef992fe-69eb-42d9-9cd6-a674cbda049b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0efdcb-23db-437c-912b-13a57a63e318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 12:32:52.701884: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8339e8-d082-4f5f-bdd7-7485b72d252a",
   "metadata": {},
   "source": [
    "#### Confirm that tensorflow is actually using a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9907a81c-f3bf-48c6-9774-bddafb8e193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n",
      "Available devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "Number of GPUs available: 0\n",
      "No GPUs available\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Print TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# List all available physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\")\n",
    "for device in physical_devices:\n",
    "    print(device)\n",
    "\n",
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"Number of GPUs available: {len(gpus)}\")\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPUs:\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPUs available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac91389",
   "metadata": {},
   "source": [
    "# Par 1 - Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3aef84",
   "metadata": {},
   "source": [
    "## Preprocessing the Training Set\n",
    "This is done by applying some **transformations** on the training set to avoid overfitting.\n",
    "\n",
    "**What are the transformations?**\n",
    "* Image augmentation\n",
    "\n",
    "### The ImageDataGenerator class\n",
    "\n",
    "The ImageDataGenerator class in Keras is a powerful tool for performing real-time data augmentation and preprocessing on images. It allows you to apply various transformations to your images such as rescaling, shearing, and zooming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a7668",
   "metadata": {},
   "source": [
    "### Create an instance of the ImageDataGenerator class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16baf703",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,          # Rescale pixel values from [0, 255] to [0, 1]\n",
    "    shear_range=0.2,          # Shear intensity (shear angle in counter-clockwise direction as radians)\n",
    "    zoom_range=0.2,           # Range for random zoom\n",
    "    horizontal_flip=True      # Randomly flip inputs horizontally\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74352b47",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "**rescale=1.0/255:** Rescales the pixel values of the images to the range [0, 1].\n",
    "\n",
    "**shear_range=0.2:** Applies random shearing transformations to the images.\n",
    "\n",
    "**zoom_range=0.2:** Applies random zoom transformations to the images.\n",
    "\n",
    "**horizontal_flip=True:** Randomly flips the images horizontally.\n",
    "\n",
    "**flow_from_directory:** Generates batches of augmented image data from the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce52a2",
   "metadata": {},
   "source": [
    "### Generate batches of augmented image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa51fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory path where the images are stored\n",
    "image_directory = \"dataset/training_set/\"\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    image_directory,\n",
    "    target_size=(150, 150),   # Resize images to 150x150 (This made the training time ver long)\n",
    "                              # Try with 64x64\n",
    "    batch_size=32,            # Number of images to yield per batch\n",
    "    class_mode='binary'       # Type of label arrays (binary/multiclass) -- (cat/dog - binary will make sense)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5a057",
   "metadata": {},
   "source": [
    "## Preprocessing teh Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f9fec99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the ImageDataGenerator for test with only rescaling\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255           # Rescale pixel values from [0, 255] to [0, 1]\n",
    ")\n",
    "\n",
    "# Directory path where the test images are stored\n",
    "test_image_directory = \"dataset/test_set/\"\n",
    "\n",
    "# Generate batches of test image data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    test_image_directory,\n",
    "    target_size=(150, 150),   # Resize images to 64x64\n",
    "    batch_size=32,            # Number of images to yield per batch\n",
    "    class_mode='binary',      # Type of label arrays (binary/multiclass)\n",
    "    shuffle=False             # Do not shuffle the data for evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebc576",
   "metadata": {},
   "source": [
    "# Part 2 - Building the Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68457a83",
   "metadata": {},
   "source": [
    "#### Initializing teh CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a6e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bdbacb5-89d6-44ae-8a10-f6fb250f107a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential, built=False>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c99145",
   "metadata": {},
   "source": [
    "### Step 1 - Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6767ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input\n",
    "\n",
    "# Add an Input layer\n",
    "cnn.add(Input(shape=(150, 150, 3)))\n",
    "\n",
    "# First convolutional layer\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058a4b6",
   "metadata": {},
   "source": [
    "### Step 2 - Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "050b1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPooling2D(pool_size=(2,2), strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884a2f9",
   "metadata": {},
   "source": [
    "### Adding a second Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59bef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolutional layer\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3e807",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff21d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# Flatten the results to feed into a DNN\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f631e9e",
   "metadata": {},
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5af158da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 12:32:54.022810: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 84934656 exceeds 10% of free system memory.\n",
      "2024-07-31 12:32:54.158982: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 84934656 exceeds 10% of free system memory.\n",
      "2024-07-31 12:32:54.192414: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 84934656 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "cnn.add(Dense(units=512, activation='relu')) # The hidden neuron can be, units=128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb30c26c",
   "metadata": {},
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "373d52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "cnn.add(Dense(units=1, activation='sigmoid')) # Binary classification, for 'dog' and 'cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ba500",
   "metadata": {},
   "source": [
    "# Part 3 - Training the Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95675b",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adef90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For binary classifiation, loss = \"binary_crossentropy\"\n",
    "### For Non-binary classification, loss = \"categorical_crossentropy\"\n",
    "\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e3af8",
   "metadata": {},
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42224540-e1c7-40ab-a28a-233cfa68e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 12:32:57.758952: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 84934656 exceeds 10% of free system memory.\n",
      "/home/agbor/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2024-07-31 12:33:02.914909: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 89718784 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 1s/step - accuracy: 0.5155 - loss: 1.1974 - val_accuracy: 0.5600 - val_loss: 0.6869\n",
      "Epoch 2/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 1s/step - accuracy: 0.5811 - loss: 0.6778 - val_accuracy: 0.6805 - val_loss: 0.6165\n",
      "Epoch 3/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 1s/step - accuracy: 0.6640 - loss: 0.6229 - val_accuracy: 0.6950 - val_loss: 0.6054\n",
      "Epoch 4/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 1s/step - accuracy: 0.6952 - loss: 0.5910 - val_accuracy: 0.7060 - val_loss: 0.5776\n",
      "Epoch 5/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 1s/step - accuracy: 0.7304 - loss: 0.5469 - val_accuracy: 0.7420 - val_loss: 0.5314\n",
      "Epoch 6/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 1s/step - accuracy: 0.7458 - loss: 0.5218 - val_accuracy: 0.7230 - val_loss: 0.5619\n",
      "Epoch 7/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 1s/step - accuracy: 0.7658 - loss: 0.4883 - val_accuracy: 0.7760 - val_loss: 0.4964\n",
      "Epoch 8/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 1s/step - accuracy: 0.7851 - loss: 0.4542 - val_accuracy: 0.7680 - val_loss: 0.4684\n",
      "Epoch 9/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 1s/step - accuracy: 0.8067 - loss: 0.4207 - val_accuracy: 0.7825 - val_loss: 0.4916\n",
      "Epoch 10/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 1s/step - accuracy: 0.8189 - loss: 0.3967 - val_accuracy: 0.7835 - val_loss: 0.4918\n",
      "Epoch 11/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 1s/step - accuracy: 0.8305 - loss: 0.3734 - val_accuracy: 0.7880 - val_loss: 0.4853\n",
      "Epoch 12/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 1s/step - accuracy: 0.8463 - loss: 0.3492 - val_accuracy: 0.7755 - val_loss: 0.5196\n",
      "Epoch 13/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 1s/step - accuracy: 0.8512 - loss: 0.3383 - val_accuracy: 0.7910 - val_loss: 0.4877\n",
      "Epoch 14/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 1s/step - accuracy: 0.8758 - loss: 0.3028 - val_accuracy: 0.7930 - val_loss: 0.4977\n",
      "Epoch 15/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 1s/step - accuracy: 0.8744 - loss: 0.3053 - val_accuracy: 0.8025 - val_loss: 0.4975\n",
      "Epoch 16/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 1s/step - accuracy: 0.8924 - loss: 0.2650 - val_accuracy: 0.7855 - val_loss: 0.5392\n",
      "Epoch 17/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 1s/step - accuracy: 0.9070 - loss: 0.2284 - val_accuracy: 0.7890 - val_loss: 0.5547\n",
      "Epoch 18/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 1s/step - accuracy: 0.9085 - loss: 0.2223 - val_accuracy: 0.7905 - val_loss: 0.5493\n",
      "Epoch 19/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 1s/step - accuracy: 0.9210 - loss: 0.1986 - val_accuracy: 0.7935 - val_loss: 0.5331\n",
      "Epoch 20/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 1s/step - accuracy: 0.9167 - loss: 0.2016 - val_accuracy: 0.7980 - val_loss: 0.5626\n",
      "Epoch 21/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 1s/step - accuracy: 0.9358 - loss: 0.1743 - val_accuracy: 0.8020 - val_loss: 0.6102\n",
      "Epoch 22/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 1s/step - accuracy: 0.9308 - loss: 0.1866 - val_accuracy: 0.7960 - val_loss: 0.6514\n",
      "Epoch 23/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 1s/step - accuracy: 0.9392 - loss: 0.1626 - val_accuracy: 0.8065 - val_loss: 0.6357\n",
      "Epoch 24/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 1s/step - accuracy: 0.9489 - loss: 0.1415 - val_accuracy: 0.8070 - val_loss: 0.6448\n",
      "Epoch 25/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 1s/step - accuracy: 0.9482 - loss: 0.1374 - val_accuracy: 0.7980 - val_loss: 0.6441\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = cnn.fit(\n",
    "    x=training_set,\n",
    "    epochs=25,\n",
    "    validation_data=test_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e224a",
   "metadata": {},
   "source": [
    "# Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "079aa9f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 64, 3), dtype=float32). Expected shape (None, 150, 150, 3), but input has incompatible shape (32, 64, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 64, 3), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m test_imagest_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(test_image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m### Predicting the result\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m training_set\u001b[38;5;241m.\u001b[39mclass_indices\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/keras/src/models/functional.py:244\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(32, 64, 3), dtype=float32). Expected shape (None, 150, 150, 3), but input has incompatible shape (32, 64, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 64, 3), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from keras.preprocessing import image\n",
    "\n",
    "# image_path = \"dataset/single_prediction/cat_or_dog_1.png\"\n",
    "# test_image = image.load_img(image_path, target_size=(64, 64))\n",
    "\n",
    "# ### Convert this test_image into an array\n",
    "# test_image = image.img_to_array(test_image)\n",
    "\n",
    "# ### Adding a batch_size to the image\n",
    "# test_imagest_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# ### Predicting the result\n",
    "# result = cnn.predict(test_image)\n",
    "\n",
    "# training_set.class_indices\n",
    "\n",
    "# if result[0][0] == 1:\n",
    "#     prediction = \"dog\"\n",
    "# else:\n",
    "#     prediction = \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the image and resize it to 64x64\n",
    "image_path = \"dataset/single_prediction/cat_or_dog_6.png\"\n",
    "\n",
    "test_image = image.load_img(image_path, target_size=(150, 150))\n",
    "test_image = image.img_to_array(test_image)\n",
    "\n",
    "# Expand dimensions to add the batch size\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Predicting the result\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "training_set.class_indices\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    prediction = \"Class Dog\"\n",
    "else:\n",
    "    prediction = \"Class Cat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ef252-41df-48fe-bc0f-03ab50a617c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518453a-9a87-4305-83cd-4aa5d54a6941",
   "metadata": {},
   "source": [
    "### Evaluate the model on a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60875d4b-5870-4cc2-8a83-9e58e7973cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, test_generator):\n",
    "    # Predict the results for the test set\n",
    "    test_images, test_labels = next(test_generator)  # Or loop through your test data\n",
    "    predictions = model.predict(test_images)\n",
    "    predicted_classes = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Print the classification report\n",
    "    print(classification_report(test_labels, predicted_classes))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(confusion_matrix(test_labels, predicted_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f105cb-6a13-42f9-8e82-88796fde305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = cnn.evaluate(test_set, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5700f7f4-ba0a-4cce-a3b6-afbcde1df299",
   "metadata": {},
   "source": [
    "### Evaluate Performance Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628cdf7-d1b4-41c5-90e0-70f5f636eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Generate predictions\n",
    "test_images, test_labels = next(test_set)  # Or loop through your test data\n",
    "predictions = cnn.predict(test_images)\n",
    "predicted_classes = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(test_labels, predicted_classes, target_names=['Dog', 'Cat']))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(confusion_matrix(test_labels, predicted_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230c271-91ad-471f-9f6e-16e0f63c9c6c",
   "metadata": {},
   "source": [
    "### Visualize Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb886e-a0f6-4660-85f2-e59522d9bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(image_paths, predictions):\n",
    "    for img_path, pred in zip(image_paths, predictions):\n",
    "        img = image.load_img(img_path, target_size=(150, 150))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Prediction: {'Dog' if pred > 0.5 else 'Cat'}\")\n",
    "        plt.show()\n",
    "\n",
    "# Assuming you have paths and predictions\n",
    "test_images = \"dataset/test_set/\"\n",
    "predictions = cnn.predict(test_images)\n",
    "plot_predictions(test_image_paths, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b959096-0030-4bd7-b1f1-488349f52015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
